\cvprsection{AI 系统中的偏见类型与成因}

AI 系统的偏见可能在数据采集、模型训练、部署使用等多个阶段产生，主要偏见类型包括：

\begin{itemize}
	\item \textbf{表示偏见（Representation Bias）}：训练数据中不同群体的样本分布不均，少数群体样本不足，导致模型在这些群体上的性能下降。
	\item \textbf{历史偏见（Historical Bias）}：数据反映了现实社会中的已有歧视结构，导致模型继承甚至放大这些偏见。
	\item \textbf{测量偏见（Measurement Bias）}：不同群体的特征采集方式存在系统性差异，影响数据的客观性和一致性。
	\item \textbf{归纳偏见（Inductive Bias）}：模型设计时引入的假设或偏好，影响学习过程和结果。
	\item \textbf{算法偏见（Algorithmic Bias）}：算法优化目标或约束条件引发的偏差，例如过度追求整体准确率忽视群体间差异。
\end{itemize}

例如，Amazon 的招聘 AI 系统曾因训练数据仅基于过去历史简历记录，导致模型倾向于选择男性候选人，这种历史偏见严重影响系统的公平性\cite{Mehrabi2021Survey}。

偏见的产生机制复杂且相互叠加，任何一个阶段的偏见都可能影响最终决策，甚至导致严重的社会不公。因此，理解偏见来源对于公平 AI 的设计至关重要。